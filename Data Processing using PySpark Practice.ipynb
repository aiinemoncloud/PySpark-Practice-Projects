{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e595701-60aa-4b36-8c77-85786ea85573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This practice project focuses on data transformation and integration using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14498be5-8316-4e6d-8caf-8adc4109daf4",
   "metadata": {},
   "source": [
    "# Setting up Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327fba16-0196-4f97-a63b-4959acd7dcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\user\\anaconda3\\lib\\site-packages (3.2)\n",
      "Requirement already satisfied: pyspark in c:\\users\\user\\anaconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: findspark in c:\\users\\user\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "# installing required packages\n",
    "!pip install wget pyspark findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58216b4a-d82e-4ff1-b483-0db426e1aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init() # initiates a spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a077f3ae-4364-46bf-b670-53a12a4145a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is the Spark API for Python we use PySpark to initialize the SparkContext.   \n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7f9b5f-cbe5-4cb9-b999-30e089bd654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a SparkContext object\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a Spark Session\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b4a781d-9ed8-4cb3-9d29-0878c9960539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset2 (1).csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download datasets\n",
    "import wget\n",
    "\n",
    "link_to_data1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n",
    "wget.download(link_to_data1)\n",
    "\n",
    "link_to_data2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n",
    "wget.download(link_to_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b739cf8b-039c-4ff7-94ef-f7bf93916c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data into a pyspark dataframe\n",
    "    \n",
    "df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756bfd7e-520c-43bf-a7f7-c1fbc362ba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date_column: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the schema of df1 and df2\n",
    "    \n",
    "df1.printSchema()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760ef90-5cfc-496c-9db0-f3e9b5c8628c",
   "metadata": {},
   "source": [
    "# Performing some transformations on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77b6e13c-19d9-46b9-bd7e-4c8dfed1e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, quarter, to_date\n",
    "\n",
    "# Add new column 'year' to df1\n",
    "df1 = df1.withColumn('year', year(to_date('date_column', 'dd/MM/yyyy')))\n",
    "\n",
    "# Add new column 'quarter' to df2    \n",
    "df2 = df2.withColumn('quarter', quarter(to_date('transaction_date', 'dd/MM/yyyy')))\n",
    "\n",
    "# this adds new date columns with the proper spark format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2db7a5c4-9781-42c5-89ff-73e852d08ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+\n",
      "|customer_id|date_column|transaction_amount|year|\n",
      "+-----------+-----------+------------------+----+\n",
      "|          1|   1/1/2022|              5000|2022|\n",
      "|          2|  15/2/2022|              1200|2022|\n",
      "|          3|  20/3/2022|               800|2022|\n",
      "|          4|  10/4/2022|              3000|2022|\n",
      "|          5|   5/5/2022|              6000|2022|\n",
      "|          6|  10/6/2022|              4500|2022|\n",
      "|          7|  15/7/2022|               200|2022|\n",
      "|          8|  20/8/2022|              3500|2022|\n",
      "|          9|  25/9/2022|               700|2022|\n",
      "|         10| 30/10/2022|              1800|2022|\n",
      "|         11|  5/11/2022|              2200|2022|\n",
      "|         12| 10/12/2022|               900|2022|\n",
      "|         13|  15/1/2023|              4800|2023|\n",
      "|         14|  20/2/2023|               300|2023|\n",
      "|         15|  25/3/2023|              4200|2023|\n",
      "|         16|  30/4/2023|              2600|2023|\n",
      "|         17|   5/5/2023|               700|2023|\n",
      "|         18|  10/6/2023|              1500|2023|\n",
      "|         19|  15/7/2023|              3200|2023|\n",
      "|         20|  20/8/2023|              1000|2023|\n",
      "+-----------+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+----------------+-----------------+-------+\n",
      "|customer_id|transaction_date|transaction_value|quarter|\n",
      "+-----------+----------------+-----------------+-------+\n",
      "|          1|        1/1/2022|             1500|      1|\n",
      "|          2|       15/2/2022|             2000|      1|\n",
      "|          3|       20/3/2022|             1000|      1|\n",
      "|          4|       10/4/2022|             2500|      2|\n",
      "|          5|        5/5/2022|             1800|      2|\n",
      "|          6|       10/6/2022|             1200|      2|\n",
      "|          7|       15/7/2022|              700|      3|\n",
      "|          8|       20/8/2022|             3000|      3|\n",
      "|          9|       25/9/2022|              600|      3|\n",
      "|         10|      30/10/2022|             1200|      4|\n",
      "|         11|       5/11/2022|             1500|      4|\n",
      "|         12|      10/12/2022|              800|      4|\n",
      "|         13|       15/1/2023|             2000|      1|\n",
      "|         14|       20/2/2023|              700|      1|\n",
      "|         15|       25/3/2023|             1800|      1|\n",
      "|         16|       30/4/2023|             1000|      2|\n",
      "|         17|        5/5/2023|              400|      2|\n",
      "|         18|       10/6/2023|             1500|      2|\n",
      "|         19|       15/7/2023|             3000|      3|\n",
      "|         20|       20/8/2023|              600|      3|\n",
      "+-----------+----------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set Spark to use the legacy time parser\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Now this should work\n",
    "df1.show()\n",
    "df2.show()\n",
    "# view the data so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29a75bbb-28ee-42cf-bad1-0b9ea65a3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns in both dataframes\n",
    "df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n",
    "# Renaming columns in both dataframes\n",
    "df2 = df2.withColumnRenamed('value', 'transaction_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31e566b7-dbb2-41ac-bfd9-068ca92a5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns description and loction from df1\n",
    "df1 = df1.drop('description', 'location')\n",
    "\n",
    "# dropping notes from df2\n",
    "df2 = df2.drop('notes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "054c4993-b774-4747-b285-393e325039bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframes based on common collumn customer_id\n",
    "joined_df = df1.join(df2, 'customer_id', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3970630c-11b5-40a1-8abd-c10260b079b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value|quarter|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|          1|   1/1/2022|              5000|2022|        1/1/2022|             1500|      1|\n",
      "|          2|  15/2/2022|              1200|2022|       15/2/2022|             2000|      1|\n",
      "|          3|  20/3/2022|               800|2022|       20/3/2022|             1000|      1|\n",
      "|          4|  10/4/2022|              3000|2022|       10/4/2022|             2500|      2|\n",
      "|          5|   5/5/2022|              6000|2022|        5/5/2022|             1800|      2|\n",
      "|          6|  10/6/2022|              4500|2022|       10/6/2022|             1200|      2|\n",
      "|          7|  15/7/2022|               200|2022|       15/7/2022|              700|      3|\n",
      "|          8|  20/8/2022|              3500|2022|       20/8/2022|             3000|      3|\n",
      "|          9|  25/9/2022|               700|2022|       25/9/2022|              600|      3|\n",
      "|         10| 30/10/2022|              1800|2022|      30/10/2022|             1200|      4|\n",
      "|         11|  5/11/2022|              2200|2022|       5/11/2022|             1500|      4|\n",
      "|         12| 10/12/2022|               900|2022|      10/12/2022|              800|      4|\n",
      "|         13|  15/1/2023|              4800|2023|       15/1/2023|             2000|      1|\n",
      "|         14|  20/2/2023|               300|2023|       20/2/2023|              700|      1|\n",
      "|         15|  25/3/2023|              4200|2023|       25/3/2023|             1800|      1|\n",
      "|         16|  30/4/2023|              2600|2023|       30/4/2023|             1000|      2|\n",
      "|         17|   5/5/2023|               700|2023|        5/5/2023|              400|      2|\n",
      "|         18|  10/6/2023|              1500|2023|       10/6/2023|             1500|      2|\n",
      "|         19|  15/7/2023|              3200|2023|       15/7/2023|             3000|      3|\n",
      "|         20|  20/8/2023|              1000|2023|       20/8/2023|              600|      3|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84d3ca5d-1b6a-4281-b155-d00fb433ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering data based on a condition \n",
    "# Foe example filtering joinedf for only transactions where amount is greater than 1000\n",
    "filtered_df = joined_df.filter('transaction_amount > 1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c60b2cc-01b5-4fa1-a1b8-594644411940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value|quarter|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|          1|   1/1/2022|              5000|2022|        1/1/2022|             1500|      1|\n",
      "|          2|  15/2/2022|              1200|2022|       15/2/2022|             2000|      1|\n",
      "|          4|  10/4/2022|              3000|2022|       10/4/2022|             2500|      2|\n",
      "|          5|   5/5/2022|              6000|2022|        5/5/2022|             1800|      2|\n",
      "|          6|  10/6/2022|              4500|2022|       10/6/2022|             1200|      2|\n",
      "|          8|  20/8/2022|              3500|2022|       20/8/2022|             3000|      3|\n",
      "|         10| 30/10/2022|              1800|2022|      30/10/2022|             1200|      4|\n",
      "|         11|  5/11/2022|              2200|2022|       5/11/2022|             1500|      4|\n",
      "|         13|  15/1/2023|              4800|2023|       15/1/2023|             2000|      1|\n",
      "|         15|  25/3/2023|              4200|2023|       25/3/2023|             1800|      1|\n",
      "|         16|  30/4/2023|              2600|2023|       30/4/2023|             1000|      2|\n",
      "|         18|  10/6/2023|              1500|2023|       10/6/2023|             1500|      2|\n",
      "|         19|  15/7/2023|              3200|2023|       15/7/2023|             3000|      3|\n",
      "|         21|  25/9/2023|              5500|2023|       25/9/2023|             2500|      3|\n",
      "|         22| 30/10/2023|              1200|2023|      30/10/2023|              700|      4|\n",
      "|         24| 10/12/2023|              2400|2023|      10/12/2023|             1200|      4|\n",
      "|         25|  15/1/2024|              1800|2024|       15/1/2024|              800|      1|\n",
      "|         27|  25/3/2024|              4200|2024|       25/3/2024|             1800|      1|\n",
      "|         28|  30/4/2024|              2600|2024|       30/4/2024|             1000|      2|\n",
      "|         30|  10/6/2024|              1500|2024|       10/6/2024|             1500|      2|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88e3eb17-4716-4d9c-88ef-cf389a1c376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data by customer \n",
    "from pyspark.sql.functions import sum \n",
    "# grouping by customer id and aggregating the sum of transaction amount \n",
    "\n",
    "total_amount_per_customer = filtered_df.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f94d0d0-16f9-4b95-8ba7-ea7ce3e048ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|total_amount|\n",
      "+-----------+------------+\n",
      "|         31|        3200|\n",
      "|         85|        1800|\n",
      "|         78|        1500|\n",
      "|         34|        1200|\n",
      "|         81|        5500|\n",
      "|         28|        2600|\n",
      "|         76|        2600|\n",
      "|         27|        4200|\n",
      "|         91|        3200|\n",
      "|         22|        1200|\n",
      "|         93|        5500|\n",
      "|          1|        5000|\n",
      "|         52|        2600|\n",
      "|         13|        4800|\n",
      "|          6|        4500|\n",
      "|         16|        2600|\n",
      "|         40|        2600|\n",
      "|         94|        1200|\n",
      "|         57|        5500|\n",
      "|         54|        1500|\n",
      "+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_amount_per_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca4e03fc-c2ac-44ee-bb30-71efa1f6ed4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o90.saveAsTable.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:119)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:159)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$catalog$1(BaseSessionStateBuilder.scala:154)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:520)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:652)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:582)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Writing the Result to a Hive table \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m total_amount_per_customer\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_totals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o90.saveAsTable.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:119)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:159)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$catalog$1(BaseSessionStateBuilder.scala:154)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:520)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:652)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:582)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# Writing the Result to a Hive table \n",
    "total_amount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb81a4f-aaf1-469d-ab02-688d12d63df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
